ssh://caoyc@210.28.133.13:20080/home/data_ti5_d/anaconda3/envs/cyc_new/bin/python -u /home/data_ti4_c/caoyc/ChID/code/DomainNER/K-BERT/run_kbert_ner.py
/home/data_ti4_c/caoyc/ChID/code/DomainNER/K-BERT
Labels:  {'[PAD]': 0, '[ENT]': 1, 'O': 2, 'B-LOC': 3, 'I-LOC': 4, 'B-PER': 5, 'I-PER': 6, 'B-ORG': 7, 'I-ORG': 8}
Vocabulary file line 344 has bad format token
Vocabulary Size:  21128
[KnowledgeGraph] Loading spo from /home/data_ti4_c/caoyc/ChID/code/DomainNER/K-BERT/brain/kgs/CnDbpedia.spo
Start training.
/home/data_ti4_c/caoyc/ChID/code/DomainNER/K-BERT/run_kbert_ner.py:363: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index
  vm_ids = torch.BoolTensor([ins[4] for ins in instances])
Batch size:  16
The number of training instances: 20864
/home/data_ti4_c/caoyc/ChID/code/DomainNER/K-BERT/uer/utils/optimizers.py:123: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)
  next_m.mul_(beta1).add_(1 - beta1, grad)
Epoch id: 1, Training steps: 100, Avg loss: 0.844
Epoch id: 1, Training steps: 200, Avg loss: 0.197
Epoch id: 1, Training steps: 300, Avg loss: 0.097
Epoch id: 1, Training steps: 400, Avg loss: 0.069
Epoch id: 1, Training steps: 500, Avg loss: 0.069
Epoch id: 1, Training steps: 600, Avg loss: 0.051
Epoch id: 1, Training steps: 700, Avg loss: 0.056
Epoch id: 1, Training steps: 800, Avg loss: 0.056
Epoch id: 1, Training steps: 900, Avg loss: 0.041
Epoch id: 1, Training steps: 1000, Avg loss: 0.046
Epoch id: 1, Training steps: 1100, Avg loss: 0.036
Epoch id: 1, Training steps: 1200, Avg loss: 0.033
Epoch id: 1, Training steps: 1300, Avg loss: 0.030
Start evaluate on dev dataset.
/home/data_ti4_c/caoyc/ChID/code/DomainNER/K-BERT/run_kbert_ner.py:270: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index
  vm_ids = torch.BoolTensor([sample[4] for sample in dataset])
Report precision, recall, and f1:
0.932, 0.937, 0.934
Start evaluation on test dataset.
Batch size:  16
The number of test instances: 4636
Report precision, recall, and f1:
0.926, 0.938, 0.932
Epoch id: 2, Training steps: 100, Avg loss: 0.036
Epoch id: 2, Training steps: 200, Avg loss: 0.032
Epoch id: 2, Training steps: 300, Avg loss: 0.025
Epoch id: 2, Training steps: 400, Avg loss: 0.023
Epoch id: 2, Training steps: 500, Avg loss: 0.020
Epoch id: 2, Training steps: 600, Avg loss: 0.018
Epoch id: 2, Training steps: 700, Avg loss: 0.019
Epoch id: 2, Training steps: 800, Avg loss: 0.016
Epoch id: 2, Training steps: 900, Avg loss: 0.016
Epoch id: 2, Training steps: 1000, Avg loss: 0.018
Epoch id: 2, Training steps: 1100, Avg loss: 0.017
Epoch id: 2, Training steps: 1200, Avg loss: 0.016
Epoch id: 2, Training steps: 1300, Avg loss: 0.012
Start evaluate on dev dataset.
Report precision, recall, and f1:
0.952, 0.954, 0.953
Start evaluation on test dataset.
Batch size:  16
The number of test instances: 4636
Report precision, recall, and f1:
0.935, 0.947, 0.941
Epoch id: 3, Training steps: 100, Avg loss: 0.016
Epoch id: 3, Training steps: 200, Avg loss: 0.016
Epoch id: 3, Training steps: 300, Avg loss: 0.014
Epoch id: 3, Training steps: 400, Avg loss: 0.015
Epoch id: 3, Training steps: 500, Avg loss: 0.011
Epoch id: 3, Training steps: 600, Avg loss: 0.009
Epoch id: 3, Training steps: 700, Avg loss: 0.009
Epoch id: 3, Training steps: 800, Avg loss: 0.010
Epoch id: 3, Training steps: 900, Avg loss: 0.008
Epoch id: 3, Training steps: 1000, Avg loss: 0.008
Epoch id: 3, Training steps: 1100, Avg loss: 0.009
Epoch id: 3, Training steps: 1200, Avg loss: 0.009
Epoch id: 3, Training steps: 1300, Avg loss: 0.006
Start evaluate on dev dataset.
Report precision, recall, and f1:
0.961, 0.958, 0.960
Start evaluation on test dataset.
Batch size:  16
The number of test instances: 4636
Report precision, recall, and f1:
0.948, 0.954, 0.951
Epoch id: 4, Training steps: 100, Avg loss: 0.009
Epoch id: 4, Training steps: 200, Avg loss: 0.009
Epoch id: 4, Training steps: 300, Avg loss: 0.009
Epoch id: 4, Training steps: 400, Avg loss: 0.008
Epoch id: 4, Training steps: 500, Avg loss: 0.006
Epoch id: 4, Training steps: 600, Avg loss: 0.006
Epoch id: 4, Training steps: 700, Avg loss: 0.005
Epoch id: 4, Training steps: 800, Avg loss: 0.006
Epoch id: 4, Training steps: 900, Avg loss: 0.005
Epoch id: 4, Training steps: 1000, Avg loss: 0.006
Epoch id: 4, Training steps: 1100, Avg loss: 0.006
Epoch id: 4, Training steps: 1200, Avg loss: 0.005
Epoch id: 4, Training steps: 1300, Avg loss: 0.004
Start evaluate on dev dataset.
Report precision, recall, and f1:
0.968, 0.957, 0.963
Start evaluation on test dataset.
Batch size:  16
The number of test instances: 4636
Report precision, recall, and f1:
0.955, 0.953, 0.954
Epoch id: 5, Training steps: 100, Avg loss: 0.005
Epoch id: 5, Training steps: 200, Avg loss: 0.005
Epoch id: 5, Training steps: 300, Avg loss: 0.004
Epoch id: 5, Training steps: 400, Avg loss: 0.005
Epoch id: 5, Training steps: 500, Avg loss: 0.004
Epoch id: 5, Training steps: 600, Avg loss: 0.004
Epoch id: 5, Training steps: 700, Avg loss: 0.003
Epoch id: 5, Training steps: 800, Avg loss: 0.004
Epoch id: 5, Training steps: 900, Avg loss: 0.002
Epoch id: 5, Training steps: 1000, Avg loss: 0.004
Epoch id: 5, Training steps: 1100, Avg loss: 0.004
Epoch id: 5, Training steps: 1200, Avg loss: 0.003
Epoch id: 5, Training steps: 1300, Avg loss: 0.003
Start evaluate on dev dataset.
Report precision, recall, and f1:
0.964, 0.960, 0.962
Start evaluation on test dataset.
Batch size:  16
The number of test instances: 4636
Report precision, recall, and f1:
0.950, 0.955, 0.952
Final evaluation on test dataset.
Batch size:  16
The number of test instances: 4636
Report precision, recall, and f1:
0.955, 0.953, 0.954

Process finished with exit code 0








ssh://caoyc@210.28.133.13:20080/home/data_ti5_d/anaconda3/envs/cyc_new/bin/python -u /home/data_ti4_c/caoyc/ChID/code/DomainNER/K-BERT/run_kbert_ner.py
/home/data_ti4_c/caoyc/ChID/code/DomainNER/K-BERT
Labels:  {'[PAD]': 0, '[ENT]': 1, 'O': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-PER': 5, 'I-PER': 6, 'B-POS': 7, 'I-POS': 8}
Vocabulary file line 344 has bad format token
Vocabulary Size:  21128
[KnowledgeGraph] Loading spo from /home/data_ti4_c/caoyc/ChID/code/DomainNER/K-BERT/brain/kgs/CnDbpedia.spo
Start training.
Batch size:  16
The number of training instances: 23675
/home/data_ti4_c/caoyc/ChID/code/DomainNER/K-BERT/uer/utils/optimizers.py:123: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)
  next_m.mul_(beta1).add_(1 - beta1, grad)
Epoch id: 1, Training steps: 100, Avg loss: 0.855
Epoch id: 1, Training steps: 200, Avg loss: 0.259
Epoch id: 1, Training steps: 300, Avg loss: 0.160
Epoch id: 1, Training steps: 400, Avg loss: 0.126
Epoch id: 1, Training steps: 500, Avg loss: 0.105
Epoch id: 1, Training steps: 600, Avg loss: 0.121
Epoch id: 1, Training steps: 700, Avg loss: 0.094
Epoch id: 1, Training steps: 800, Avg loss: 0.070
Epoch id: 1, Training steps: 900, Avg loss: 0.089
Epoch id: 1, Training steps: 1000, Avg loss: 0.068
Epoch id: 1, Training steps: 1100, Avg loss: 0.096
Epoch id: 1, Training steps: 1200, Avg loss: 0.094
Epoch id: 1, Training steps: 1300, Avg loss: 0.112
Epoch id: 1, Training steps: 1400, Avg loss: 0.087
Start evaluate on dev dataset.
Report precision, recall, and f1:
0.840, 0.849, 0.844
Start evaluation on test dataset.
Batch size:  16
The number of test instances: 3066
Report precision, recall, and f1:
0.844, 0.860, 0.852
Epoch id: 2, Training steps: 100, Avg loss: 0.159
Epoch id: 2, Training steps: 200, Avg loss: 0.094
Epoch id: 2, Training steps: 300, Avg loss: 0.094
Epoch id: 2, Training steps: 400, Avg loss: 0.074
Epoch id: 2, Training steps: 500, Avg loss: 0.071
Epoch id: 2, Training steps: 600, Avg loss: 0.078
Epoch id: 2, Training steps: 700, Avg loss: 0.062
Epoch id: 2, Training steps: 800, Avg loss: 0.046
Epoch id: 2, Training steps: 900, Avg loss: 0.068
Epoch id: 2, Training steps: 1000, Avg loss: 0.052
Epoch id: 2, Training steps: 1100, Avg loss: 0.068
Epoch id: 2, Training steps: 1200, Avg loss: 0.065
Epoch id: 2, Training steps: 1300, Avg loss: 0.063
Epoch id: 2, Training steps: 1400, Avg loss: 0.065
Start evaluate on dev dataset.
Report precision, recall, and f1:
0.857, 0.867, 0.862
Start evaluation on test dataset.
Batch size:  16
The number of test instances: 3066
Report precision, recall, and f1:
0.856, 0.874, 0.865
Epoch id: 3, Training steps: 100, Avg loss: 0.114
Epoch id: 3, Training steps: 200, Avg loss: 0.074
Epoch id: 3, Training steps: 300, Avg loss: 0.074
Epoch id: 3, Training steps: 400, Avg loss: 0.064
Epoch id: 3, Training steps: 500, Avg loss: 0.061
Epoch id: 3, Training steps: 600, Avg loss: 0.063
Epoch id: 3, Training steps: 700, Avg loss: 0.047
Epoch id: 3, Training steps: 800, Avg loss: 0.039
Epoch id: 3, Training steps: 900, Avg loss: 0.058
Epoch id: 3, Training steps: 1000, Avg loss: 0.043
Epoch id: 3, Training steps: 1100, Avg loss: 0.054
Epoch id: 3, Training steps: 1200, Avg loss: 0.052
Epoch id: 3, Training steps: 1300, Avg loss: 0.050
Epoch id: 3, Training steps: 1400, Avg loss: 0.053
Start evaluate on dev dataset.
Report precision, recall, and f1:
0.865, 0.872, 0.869
Start evaluation on test dataset.
Batch size:  16
The number of test instances: 3066
Report precision, recall, and f1:
0.861, 0.878, 0.869
Epoch id: 4, Training steps: 100, Avg loss: 0.095
Epoch id: 4, Training steps: 200, Avg loss: 0.062
Epoch id: 4, Training steps: 300, Avg loss: 0.061
Epoch id: 4, Training steps: 400, Avg loss: 0.054
Epoch id: 4, Training steps: 500, Avg loss: 0.053
Epoch id: 4, Training steps: 600, Avg loss: 0.053
Epoch id: 4, Training steps: 700, Avg loss: 0.041
Epoch id: 4, Training steps: 800, Avg loss: 0.033
Epoch id: 4, Training steps: 900, Avg loss: 0.050
Epoch id: 4, Training steps: 1000, Avg loss: 0.038
Epoch id: 4, Training steps: 1100, Avg loss: 0.042
Epoch id: 4, Training steps: 1200, Avg loss: 0.043
Epoch id: 4, Training steps: 1300, Avg loss: 0.040
Epoch id: 4, Training steps: 1400, Avg loss: 0.047
Start evaluate on dev dataset.
Report precision, recall, and f1:
0.866, 0.879, 0.872
Start evaluation on test dataset.
Batch size:  16
The number of test instances: 3066
Report precision, recall, and f1:
0.861, 0.882, 0.871
Epoch id: 5, Training steps: 100, Avg loss: 0.077
Epoch id: 5, Training steps: 200, Avg loss: 0.053
Epoch id: 5, Training steps: 300, Avg loss: 0.050
Epoch id: 5, Training steps: 400, Avg loss: 0.048
Epoch id: 5, Training steps: 500, Avg loss: 0.046
Epoch id: 5, Training steps: 600, Avg loss: 0.046
Epoch id: 5, Training steps: 700, Avg loss: 0.037
Epoch id: 5, Training steps: 800, Avg loss: 0.030
Epoch id: 5, Training steps: 900, Avg loss: 0.045
Epoch id: 5, Training steps: 1000, Avg loss: 0.035
Epoch id: 5, Training steps: 1100, Avg loss: 0.036
Epoch id: 5, Training steps: 1200, Avg loss: 0.038
Epoch id: 5, Training steps: 1300, Avg loss: 0.035
Epoch id: 5, Training steps: 1400, Avg loss: 0.042
Start evaluate on dev dataset.
Report precision, recall, and f1:
0.868, 0.887, 0.877
Start evaluation on test dataset.
Batch size:  16
The number of test instances: 3066
Report precision, recall, and f1:
0.862, 0.890, 0.876
Final evaluation on test dataset.
Batch size:  16
The number of test instances: 3066
Report precision, recall, and f1:
0.862, 0.890, 0.876

Process finished with exit code 0




ssh://caoyc@210.28.133.13:20080/home/data_ti5_d/anaconda3/envs/cyc_new/bin/python -u /home/data_ti4_c/caoyc/ChID/code/DomainNER/K-BERT/run_kbert_ner.py
/home/data_ti4_c/caoyc/ChID/code/DomainNER/K-BERT
Labels:  {'[PAD]': 0, '[ENT]': 1, 'O': 2, 'B-BodyPart': 3, 'I-BodyPart': 4, 'B-Symptom': 5, 'I-Symptom': 6, 'B-Disease': 7, 'I-Disease': 8, 'B-Examine': 9, 'I-Examine': 10, 'B-Cure': 11, 'I-Cure': 12}
Vocabulary file line 344 has bad format token
Vocabulary Size:  21128
[KnowledgeGraph] Loading spo from /home/data_ti4_c/caoyc/ChID/code/DomainNER/K-BERT/brain/kgs/CnDbpedia.spo
2 GPUs are available. Let's use them.
Start training.
Batch size:  16
The number of training instances: 6919
/home/data_ti5_d/anaconda3/envs/cyc_new/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
/home/data_ti4_c/caoyc/ChID/code/DomainNER/K-BERT/uer/utils/optimizers.py:123: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)
  next_m.mul_(beta1).add_(1 - beta1, grad)
Epoch id: 1, Training steps: 100, Avg loss: 0.894
Epoch id: 1, Training steps: 200, Avg loss: 0.215
Epoch id: 1, Training steps: 300, Avg loss: 0.153
Epoch id: 1, Training steps: 400, Avg loss: 0.154
Start evaluate on dev dataset.
Report precision, recall, and f1:
0.871, 0.909, 0.889
Start evaluation on test dataset.
Batch size:  16
The number of test instances: 755
Report precision, recall, and f1:
0.903, 0.905, 0.904
Epoch id: 2, Training steps: 100, Avg loss: 0.148
Epoch id: 2, Training steps: 200, Avg loss: 0.097
Epoch id: 2, Training steps: 300, Avg loss: 0.085
Epoch id: 2, Training steps: 400, Avg loss: 0.089
Start evaluate on dev dataset.
Report precision, recall, and f1:
0.905, 0.931, 0.918
Start evaluation on test dataset.
Batch size:  16
The number of test instances: 755
Report precision, recall, and f1:
0.923, 0.927, 0.925
Epoch id: 3, Training steps: 100, Avg loss: 0.099
Epoch id: 3, Training steps: 200, Avg loss: 0.073
Epoch id: 3, Training steps: 300, Avg loss: 0.068
Epoch id: 3, Training steps: 400, Avg loss: 0.064
Start evaluate on dev dataset.
Report precision, recall, and f1:
0.912, 0.933, 0.922
Start evaluation on test dataset.
Batch size:  16
The number of test instances: 755
Report precision, recall, and f1:
0.923, 0.928, 0.926
Epoch id: 4, Training steps: 100, Avg loss: 0.075
Epoch id: 4, Training steps: 200, Avg loss: 0.062
Epoch id: 4, Training steps: 300, Avg loss: 0.057
Epoch id: 4, Training steps: 400, Avg loss: 0.052
Start evaluate on dev dataset.
Report precision, recall, and f1:
0.912, 0.937, 0.924
Start evaluation on test dataset.
Batch size:  16
The number of test instances: 755
Report precision, recall, and f1:
0.925, 0.933, 0.929
Epoch id: 5, Training steps: 100, Avg loss: 0.064
Epoch id: 5, Training steps: 200, Avg loss: 0.052
Epoch id: 5, Training steps: 300, Avg loss: 0.050
Epoch id: 5, Training steps: 400, Avg loss: 0.045
Start evaluate on dev dataset.
Report precision, recall, and f1:
0.919, 0.941, 0.930
Start evaluation on test dataset.
Batch size:  16
The number of test instances: 755
Report precision, recall, and f1:
0.931, 0.938, 0.935
Final evaluation on test dataset.
Batch size:  16
The number of test instances: 755
Report precision, recall, and f1:
0.931, 0.938, 0.935

Process finished with exit code 0



/home/data_ti4_c/caoyc/ChID/code/DomainNER/K-BERT
Labels:  {'[PAD]': 0, '[ENT]': 1, 'O': 2, 'B-BodyPart': 3, 'I-BodyPart': 4, 'B-Symptom': 5, 'I-Symptom': 6, 'B-Disease': 7, 'I-Disease': 8, 'B-Examine': 9, 'I-Examine': 10, 'B-Cure': 11, 'I-Cure': 12}
Vocabulary file line 344 has bad format token
Vocabulary Size:  21128
[KnowledgeGraph] Loading spo from /home/data_ti4_c/caoyc/ChID/code/DomainNER/K-BERT/brain/kgs/CnDbpedia.spo
Final evaluation on test dataset.
Batch size:  16
The number of test instances: 755
Report precision, recall, and f1:
2562 / 2737 = 0.9360613810741688, 2562 / 2720 = 0.9419117647058823
0.936, 0.942, 0.939



ssh://caoyc@210.28.133.13:20647/home/data_ti5_d/anaconda3/envs/cyc_new/bin/python -u /home/data_ti4_c/caoyc/ChID/code/DomainNER/K-BERT/run_kbert_ner.py
/home/data_ti4_c/caoyc/ChID/code/DomainNER/K-BERT
Labels:  {'[PAD]': 0, '[ENT]': 1, 'O': 2, 'B-BodyPart': 3, 'I-BodyPart': 4, 'B-Symptom': 5, 'I-Symptom': 6, 'B-Disease': 7, 'I-Disease': 8, 'B-Examine': 9, 'I-Examine': 10, 'B-Cure': 11, 'I-Cure': 12}
Vocabulary file line 344 has bad format token
Vocabulary Size:  21128
[KnowledgeGraph] Loading spo from /home/data_ti4_c/caoyc/ChID/code/DomainNER/K-BERT/brain/kgs/CnDbpedia.spo
Final evaluation on test dataset.
Batch size:  16
The number of test instances: 755
Report precision, recall, and f1:
2560 / 2737 = 0.9353306540007307, 2560 / 2720 = 0.9411764705882353
0.935, 0.941, 0.938

search_for_k:
-0.3:
avg_precision: 2423 / 2475 = 97.8990, avg_recall: 2423 / 2706 = 89.5418, avg_f1: 93.5341
-1:
avg_precision: 2427 / 2488 = 97.5482, avg_recall: 2427 / 2706 = 89.6896, avg_f1: 93.4540